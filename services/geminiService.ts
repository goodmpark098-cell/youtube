import { GoogleGenAI, Type, Schema } from "@google/genai";
import { AnalysisResult } from "../types";

const getClient = () => {
  const apiKey = process.env.API_KEY;
  if (!apiKey) {
    throw new Error("API Key is missing. Please check your environment variables.");
  }
  return new GoogleGenAI({ apiKey });
};

// 1. First step: Analyze structure and suggest topics
export const analyzeTranscript = async (
  originalTranscript: string
): Promise<AnalysisResult> => {
  const ai = getClient();

  const prompt = `
    ë‹¹ì‹ ì€ 100ë§Œ êµ¬ë…ìë¥¼ ë³´ìœ í•œ ìœ íŠœë¸Œ ì „ëµê°€ì…ë‹ˆë‹¤.
    
    [ì…ë ¥ëœ ëŒ€ë³¸]ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•˜ì„¸ìš”:
    1. ì´ ì˜ìƒì´ ì„±ê³µí•œ êµ¬ì¡°ì  ì´ìœ (í›„í‚¹, ì†ë„ê°, ê°ì •ì„  ë“±)ë¥¼ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.
    2. ì´ êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ì ìš©í–ˆì„ ë•Œ ëŒ€ë°•ì´ ë‚  ë§Œí•œ ì„œë¡œ ë‹¤ë¥¸ ë¶„ì•¼ì˜ ìƒˆë¡œìš´ ì£¼ì œ 3ê°€ì§€ë¥¼ ì¶”ì²œí•˜ì„¸ìš”.
    
    ì…ë ¥ëœ ëŒ€ë³¸: """${originalTranscript}"""
  `;

  const schema: Schema = {
    type: Type.OBJECT,
    properties: {
      structureSummary: {
        type: Type.STRING,
        description: "ì„±ê³µ ìš”ì¸ì— ëŒ€í•œ êµ¬ì¡°ì  ë¶„ì„ ìš”ì•½ (í•œêµ­ì–´)",
      },
      suggestedTopics: {
        type: Type.ARRAY,
        items: { type: Type.STRING },
        description: "ì´ êµ¬ì¡°ì— ì í•©í•œ ì¶”ì²œ ì£¼ì œ 3ê°€ì§€",
      },
    },
    required: ["structureSummary", "suggestedTopics"],
  };

  const response = await ai.models.generateContent({
    model: 'gemini-2.5-flash',
    contents: prompt,
    config: {
      responseMimeType: "application/json",
      responseSchema: schema,
      temperature: 0.7,
    }
  });

  const text = response.text;
  if (!text) throw new Error("ë¶„ì„ ê²°ê³¼ë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.");
  
  return JSON.parse(text) as AnalysisResult;
};

// 2. Second step: Generate the actual script based on selected topic
export const generateViralScriptStream = async (
  originalTranscript: string,
  structureSummary: string,
  newTopic: string,
  onChunk: (text: string) => void
): Promise<void> => {
  const ai = getClient();
  
  const prompt = `
    ë‹¹ì‹ ì€ ì „ë¬¸ ìœ íŠœë¸Œ ìŠ¤í¬ë¦½íŠ¸ ì‘ê°€ì…ë‹ˆë‹¤.
    
    ëª©í‘œ: 
    [ì›ë³¸ ëŒ€ë³¸]ì˜ ì„±ê³µ ê³µì‹(êµ¬ì¡°, í†¤ì•¤ë§¤ë„ˆ, í˜¸í¡)ì„ ì™„ë²½í•˜ê²Œ ë²¤ì¹˜ë§ˆí‚¹í•˜ì—¬, 
    [ìƒˆë¡œìš´ ì£¼ì œ]ì— ëŒ€í•œ ë§¤ë ¥ì ì¸ ìœ íŠœë¸Œ ëŒ€ë³¸ì„ ì‘ì„±í•˜ì„¸ìš”.
    
    ì…ë ¥ ë°ì´í„°:
    1. ì›ë³¸ ëŒ€ë³¸: """${originalTranscript}"""
    2. ì›ë³¸ êµ¬ì¡° íŠ¹ì§•: "${structureSummary}"
    3. ìƒˆë¡œìš´ ì£¼ì œ: "${newTopic}"
    
    ì‘ì„± ê°€ì´ë“œ:
    - ì–¸ì–´: ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ êµ¬ì–´ì²´ (ìœ íŠœë¸Œ ê°ì„±)
    - í˜•ì‹: ë§ˆí¬ë‹¤ìš´ (Markdown)
    - ì‹œê°ì  ì—°ì¶œ: ë³¼ë“œì²´ë¡œ ì¹´ë©”ë¼ ì•µê¸€ì´ë‚˜ B-roll ì§€ì‹œì‚¬í•­ í¬í•¨ (ì˜ˆ: **[í™”ë©´ ì „í™˜: ë¹ ë¥´ê²Œ ì§€ë‚˜ê°€ëŠ” ë„ì‹œ í’ê²½]**)
    - êµ¬ì¡°: ì›ë³¸ì˜ í›…(Hook), ë³¸ë¡  ì „ê°œ ë°©ì‹, í´ë¼ì´ë§¥ìŠ¤, CTA ìœ„ì¹˜ë¥¼ ê·¸ëŒ€ë¡œ ë”°ë¥¼ ê²ƒ.
    
    ì¶œë ¥ í˜•ì‹:
    
    ## ğŸ§¬ êµ¬ì¡° ë¶„ì„ ìš”ì•½
    *${structureSummary}*
    
    ---
    
    ## ğŸ¬ ìƒˆë¡œìš´ ëŒ€ë³¸: ${newTopic}
    *(ì—¬ê¸°ì„œë¶€í„° ëŒ€ë³¸ ì‘ì„±)*
  `;

  try {
    const responseStream = await ai.models.generateContentStream({
      model: 'gemini-2.5-flash',
      contents: prompt,
      config: {
        temperature: 0.7,
        topP: 0.8,
        topK: 40,
      }
    });

    for await (const chunk of responseStream) {
      if (chunk.text) {
        onChunk(chunk.text);
      }
    }
  } catch (error) {
    console.error("Gemini generation error:", error);
    throw error;
  }
};